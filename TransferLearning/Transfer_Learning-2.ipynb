{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwY1-Dv69fI8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtDgONFNdA7n"
      },
      "source": [
        "This is following:  A Neural Algorithm of Artistic Style\n",
        "\n",
        "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge\n",
        "\n",
        "--> https://arxiv.org/abs/1508.06576"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0oMWD-d9grN"
      },
      "source": [
        "Style transfer is an optimisation problem, used to  minimisation the loss function. Taking two images (style image and x image ), the optimisation minimises the difference betweeen the images.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Zero shot or one shot learning: machine learning with very little data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu6wl1q2_hs1"
      },
      "source": [
        "Network used is VGG16: it is an image classification convolutional network \n",
        "At each layer it apples a series of operations to the input image\n",
        "Input image: matrix of values \n",
        "\n",
        "At each layer we have a stack of filters that are learned overtime\n",
        "Filters: the filters are 3D vectors, which are a collection of matrics that are 2d, and the 3D is the rgb\n",
        "At each of these filters, a matrix multiplication and then a summation operation occurs on te input image.\n",
        "It acts as feture identifer\n",
        "large number imples a feture is detected \n",
        "If not = zero.\n",
        "\n",
        "It will output a feature map or an activation map (large matrix of values). We want to minimise the difference between our feature map and our content image.\n",
        "\n",
        "\n",
        "Content image: Image to modify \n",
        "Style image: we want to apply to content image\n",
        "Mixed Image: empty, random nosie initilase dimage we will add to over time.\n",
        "\n",
        "There is style loss and content loss and the combination of these using gradient value updates the mixed image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_RSn4IY_hGb"
      },
      "source": [
        "# import our dependencies \n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt # plot out what we see\n",
        "import tensorflow as tf  # machine learnign library \n",
        "import numpy as np   # help calculate gram matrix\n",
        "import PIL.Image    # show image we want to show"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1eZ5X43f-_x_",
        "outputId": "0eccf973-b22b-4ef9-d573-c49bfb3b87d0"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogsjQqnvCLs5"
      },
      "source": [
        "#16 layer convostuionary netwoerk. It is prerained with neccesssary filters\n",
        "# more layers means better predictions but mroe computation time \n",
        "#import vgg16.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4W8yh2nJ-a6",
        "outputId": "05fd065a-e2ee-483f-c5cd-4bcd7963ca78"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16(weights='imagenet')\n",
        "print(model.summary())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 5s 0us/step\n",
            "553476096/553467096 [==============================] - 5s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW7dd60NCiVz"
      },
      "source": [
        "#vgg16.maybe_download()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-MuuBxvKjNS"
      },
      "source": [
        "# define image helper fucntions \n",
        "\n",
        "def load_image(filename, max_size=None):\n",
        "    image = PIL.Image.open(filename)\n",
        "\n",
        "    if max_size is not None:\n",
        "        # Calculate the appropriate rescale-factor for\n",
        "        # ensuring a max height and width, while keeping\n",
        "        # the proportion between them.\n",
        "        factor = max_size / np.max(image.size)\n",
        "    \n",
        "        # Scale the image's height and width.\n",
        "        size = np.array(image.size) * factor\n",
        "\n",
        "        # The size is now floating-point because it was scaled.\n",
        "        # But PIL requires the size to be integers.\n",
        "        size = size.astype(int)\n",
        "\n",
        "        # Resize the image.\n",
        "        image = image.resize(size, PIL.Image.LANCZOS)\n",
        "\n",
        "    # Convert to numpy floating-point array.\n",
        "    return np.float32(image)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgO6WsB2KqwW"
      },
      "source": [
        "def save_image(image, filename):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "    \n",
        "    # Convert to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "    \n",
        "    # Write the image-file in jpeg-format.\n",
        "    with open(filename, 'wb') as file:\n",
        "        PIL.Image.fromarray(image).save(file, 'jpeg')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO77ylIqKw9n"
      },
      "source": [
        "def plot_image_big(image):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "\n",
        "    # Convert pixels to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Convert to a PIL-image and display it.\n",
        "    display(PIL.Image.fromarray(image))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plZ7wr9OK3_1"
      },
      "source": [
        "def plot_images(content_image, style_image, mixed_image):\n",
        "    # Create figure with sub-plots.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
        "\n",
        "    # Adjust vertical spacing.\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Use interpolation to smooth pixels?\n",
        "    smooth = True\n",
        "    \n",
        "    # Interpolation type.\n",
        "    if smooth:\n",
        "        interpolation = 'sinc'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    # Plot the content-image.\n",
        "    # Note that the pixel-values are normalized to\n",
        "    # the [0.0, 1.0] range by dividing with 255.\n",
        "    ax = axes.flat[0]\n",
        "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Content\")\n",
        "\n",
        "    # Plot the mixed-image.\n",
        "    ax = axes.flat[1]\n",
        "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Mixed\")\n",
        "\n",
        "    # Plot the style-image\n",
        "    ax = axes.flat[2]\n",
        "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Style\")\n",
        "\n",
        "    # Remove ticks from all the plots.\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHYrth0XK7tu"
      },
      "source": [
        "Loss Function \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLVZGCL3LHFT"
      },
      "source": [
        "# define mean squared error:  it is a function given to input tensors (a, b)\n",
        "# outputs a scaler --> single value\n",
        "\n",
        "# it is a hlerper fucntion inside our content loss and our style loss\n",
        "\n",
        "def mean_squared_error(a, b):\n",
        "    return tf.reduce_mean(tf.square(a - b))\n",
        "\n",
        "#it is the average of the sqaure of the difference between the output feature map and our image\n",
        "# one tensor will be our content image or style image. The other will be our feature map\n",
        "                #. --> the result of the chosen layer \n",
        "# the difeeec is subtracted, the squared then we do a reduce mean\n",
        "# square makes it a positive\n",
        "\n",
        "# reduce Mean: find the average of all vlaues inside of the matrix \n",
        "#The smaller the means squared error, the closer you are to finding the line of best fit.\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJC7t0nPMb7Z"
      },
      "source": [
        "# Creates the loss-function for the content-image.\n",
        "\n",
        "'''Parameters:\n",
        "    session: An open TensorFlow session for running the model's graph.\n",
        "    model: The model, e.g. an instance of the VGG16-class.\n",
        "    content_image: Numpy float array with the content-image.\n",
        "    layer_ids: List of integer id's for the layers to use in the model. --> will be ther higher level layers\n",
        "'''\n",
        "#you should expirment what looks good for different layers\n",
        "def create_content_loss(session, model, content_image, layer_ids):\n",
        "\n",
        "  # it is a python dictionary object that is genrateed with placeholders as keys\n",
        "  # it genrates a set of place holder values \n",
        "  # key is the image, value is the content image.\n",
        "  # for very time we want to feed anything into our compeiutuon graph -->as ditionary\n",
        "   feed_dict = model.create_feed_dict(image=content_image)\n",
        "\n",
        "    # play around with differnet layers \n",
        "   layers = model.get_layer_tensors(layer_ids)\n",
        "\n",
        "  # calaute the output values given the layers --> feed into dictoanry\n",
        "  # get outptu values for thaose layers for the conent image\n",
        "   values = session.run(layers, feed_dict=feed_dict)\n",
        "\n",
        "   with model.graph.as_default():\n",
        "     # need empty as we will claaues a collection of lossses \n",
        "     layer_losses = []\n",
        "\n",
        "\n",
        "      # For each layer and its corresponding values\n",
        "     for value, layer in model(values, layers):\n",
        "\n",
        "        # tf.cocnsstnt --> cannont change\n",
        "        # this is the content image value\n",
        "        value_const = tf.constant(value)\n",
        "\n",
        "\n",
        "        #claaclte loss using mean sqaured error\n",
        "        # inputs: value at a layer, constant value\n",
        "        #\n",
        "        loss = mean_squared_error(layer, value_const)\n",
        "\n",
        "        # add to pre inititlised list \n",
        "        layer_losses.append(loss)\n",
        "\n",
        "\n",
        "     total_loss = tf.reduce_mean(layer_losses)\n",
        "\n",
        "\n",
        "   return total_loss\n",
        "\n",
        "\n",
        "# get eman sqaure eroro of feature activtions of the given layer in th model \n",
        "# between the conent image and lmixed image --.~ when minised, it will make the mixed image that much more stylised \n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZY8xoAlMb-s"
      },
      "source": [
        "# for style we do a gram matrix\n",
        "# gram matrix : we are measuring the correlations between our feature vectors after flattening the filter image into vectors\n",
        "# it is the matrix product between our initial matirix and then its transposed ( filpping it by 90 degrees )\n",
        "# i.e multiplying a matrix by its transpose \n",
        "# this is used to minimise via the mean sqaured error\n",
        "# it is 4D tensors: collection of pixel and rgb make it 3d -->a collection of those make it 4D\n",
        "def gram_matrix(tensor):\n",
        "\n",
        "  #4d tensor from convolutional layer\n",
        "  shape = tensor.get_shape()\n",
        "\n",
        "  # each stack in a the matrox is called a channel.\n",
        "  num_channels = int(shape[3])\n",
        "\n",
        "\n",
        "  # -1 : means whatever number makes the data fit\n",
        "  # we are reshapping th edensor soe it is a 2d matrix \n",
        "  # flatten the contents of each o the feature chnnels --> so we can multiplt it \n",
        "  # akin to normalisation \n",
        "  matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
        "\n",
        "\n",
        "    # the 2-dim matrix with itself. \n",
        "    # matrix multipled by its transpose\n",
        "  gram = tf.matmul(tf.transpose(matrix), matrix)\n",
        "\n",
        "  return gram\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLzRzkkRIiD"
      },
      "source": [
        "# style loss \n",
        "# compute given the gram matrix\n",
        "\n",
        "\n",
        "def create_style_loss(session, model, style_image, layer_ids):\n",
        "  # creates place holder value for style image\n",
        "  feed_dict = model.create_feed_dict(image=style_image)\n",
        "\n",
        "  # then get layers \n",
        "  layers = model.get_layer_tensors(layer_ids)\n",
        "\n",
        "\n",
        "  with model.graph.as_default():\n",
        "\n",
        "      # this is deffernece to the content loss\n",
        "      # we take the gram matrix of a layer, instead of the regular\n",
        "      # why ? ( from crator at a confernce ): it encodes 2nd order ststisics of the set of filters --> looking at it froma higehr layer\n",
        "      # gram maxtrix , tolss away anything uncewesay to focus on the syle \n",
        "      # hence it works for style and not content \n",
        "    gram_layers = [gram_matrix(layer) for layer in layers]\n",
        "\n",
        "\n",
        "    values = session.run(gram_layers, feed_dict=feed_dict)\n",
        "\n",
        "\n",
        "    # Initialize an empty list of loss-functions.\n",
        "    layer_losses = []\n",
        "\n",
        "\n",
        "    # For each Gram-matrix at each layer and its corresponding values or their loss function.\n",
        "    for value, gram_layer in model(values, gram_layers):\n",
        "\n",
        "      value_const = tf.constant(value)\n",
        "\n",
        "\n",
        "      # where calducalting the mean sqaured error of the ground layer and the value of that ground matrix layer when inputting the style image\n",
        "      loss = mean_squared_error(gram_layer, value_const)\n",
        "\n",
        "\n",
        "      # list of loss-functions --> losses at each layer\n",
        "      layer_losses.append(loss)\n",
        "    \n",
        "    # take the average of all losess in matrix --> scalar value \n",
        "    total_loss = tf.reduce_mean(layer_losses)\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AinvnPjPSvPs"
      },
      "source": [
        "# one more loss \n",
        "# the denoising loss( totoal vartioan loss) --> when paper didnt do this image was OK\n",
        "                                          # when they did it was --> result improved\n",
        "# shifts images by one pixel on the x and y axis and calculates the diffenec btween the shifted image the original image \n",
        "# abosloute value so it is positive \n",
        "\n",
        "#--> helps supress noise in the image that was generated : its a blurriness, not that clear \n",
        "\n",
        "\n",
        "def create_denoise_loss(model):\n",
        "    loss = tf.reduce_sum(tf.abs(model.input[:,1:,:,:] - model.input[:,:-1,:,:])) + \\\n",
        "           tf.reduce_sum(tf.abs(model.input[:,:,1:,:] - model.input[:,:,:-1,:]))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B44LB9_hTmUF"
      },
      "source": [
        "# Style-transfer \n",
        "\n",
        "The main optimaisation algorithm for the Style-Transfer algorithm, tha uses graduent descent on the prevoiusly defined loss fucntion and it normalises the loss function. \n",
        "\n",
        "Each itertaion of the optimisation, the losss-values are adjusted so they indviually equal to one. This allowws for the lodd-weights to be se independtly for the chosen sytel and conent layers. \n",
        "\n",
        "These weights are also adpated during optimisation, ensuring the presevration of the desired ratio between the style, conentand denoising. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bvIWxi3Tj2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "4804b5fe-abdd-4576-ee85-fe320b1963bc"
      },
      "source": [
        "# we can modify the weigths \n",
        "# weighing the sytle more than the content\n",
        "# To:DO --> learn the best weights later with learnig algorthim \n",
        "'''\n",
        "parameters: \n",
        "    content_image: Numpy 3-dim float-array with the content-image.\n",
        "    style_image: Numpy 3-dim float-array with the style-image.\n",
        "    content_layer_ids: List of integers identifying the content-layers.\n",
        "    style_layer_ids: List of integers identifying the style-layers.\n",
        "    weight_content: Weight for the content-loss-function.\n",
        "    weight_style: Weight for the style-loss-function.\n",
        "    weight_denoise: Weight for the denoising-loss-function.\n",
        "    num_iterations: Number of optimization iterations to perform.\n",
        "    step_size: Step-size for the gradient in each iteration.\n",
        "\n",
        "'''\n",
        "def style_transfer(content_image, style_image,\n",
        "                   content_layer_ids, style_layer_ids,\n",
        "                   weight_content=1.5, weight_style=10.0, #these weights can be changed and modified\n",
        "                   weight_denoise=0.3,                      \n",
        "                   num_iterations=120, step_size=10.0):\n",
        "  \n",
        "\n",
        "  #model = vgg16.VGG16()\n",
        "\n",
        "  model = VGG16() # define the model --> 16 layer convoluntary net with \n",
        "  # a fully conetneted layer at the end: usually used for classfication \n",
        "\n",
        "  # Create a TensorFlow-session.\n",
        "  #the session always encapuates the computation graph \n",
        "  session = tf.InteractiveSession(graph=model.graph)\n",
        "\n",
        "  # Print the names of the content-layers.\n",
        "  # helps to see whats happening \n",
        "  print(\"Content layers:\")\n",
        "  print(model.get_layer_names(content_layer_ids))\n",
        "  print()\n",
        "\n",
        "\n",
        "  # Create the loss-function for the content-layers and -image.\n",
        "  # (session value, moel value, content image, layer_id is indexies) as we are trying to minimise the \n",
        "  # loss for the content\n",
        "  #layer_id: is indexies -->claeute the diffenec between the raw actiavation \n",
        "  #   at the given layers in the conenet image. This can be thought of as a matrix\n",
        "  #The output loss cotnent value is a scalar value\n",
        "\n",
        "  #Note: the gram matrix for content loss is not used, as the gram matrix is a \n",
        "  #      second order statistic. Style is more abstract than content, content is somehting that can be \n",
        "  #      pointed out and say what is the coent in an image, so the raw acvtivation in content can be taken\n",
        "  #      style is more nuramnce and emebebd and engrained in every layer.\n",
        "  #      the gram matrix creates that extra layer of abstration \n",
        "  loss_content = create_content_loss(session=session,\n",
        "                                       model=model,\n",
        "                                       content_image=content_image,\n",
        "                                       layer_ids=content_layer_ids)\n",
        "  \n",
        "   # Create the loss-function for the denoising of the mixed-image.\n",
        "   # the extra step is the gram matrix calcuation \n",
        "   # taking the raw activations, it calautates the gram matrixes from them.\n",
        "  loss_style = create_style_loss(session=session,\n",
        "                                     model=model,\n",
        "                                     style_image=style_image,\n",
        "                                     layer_ids=style_layer_ids)\n",
        "  # Create the loss-function for the denoising of the mixed-image.\n",
        "  # so it is a little less blurry\n",
        "  loss_denoise = create_denoise_loss(model)\n",
        "\n",
        "  # adjust levels of loss functions, normalize them\n",
        "  # multiply them with a variable\n",
        "  # taking reciprocal values of loss values of content, style, denoising\n",
        "  # small constant to avoid divide by 0\n",
        "  # adjustment value normalizes loss so approximately 1\n",
        "  # weights should be set relative to each other dont depend on layers\n",
        "  # we are using\n",
        "\n",
        "  # Create TensorFlow variables for adjusting the values of\n",
        "  # the loss-functions. This is explained below.\n",
        "  # create adjustement weight- adjusts how much each are weighted\n",
        "\n",
        "  adj_content = tf.Variable(1e-10, name='adj_content')\n",
        "  adj_style = tf.Variable(1e-10, name='adj_style')\n",
        "  adj_denoise = tf.Variable(1e-10, name='adj_denoise')\n",
        "\n",
        "\n",
        "  # Initialize the adjustment values for the loss-functions.\n",
        "  session.run([adj_content.initializer,\n",
        "                 adj_style.initializer,\n",
        "                 adj_denoise.initializer])\n",
        "  \n",
        "\n",
        "  # Create TensorFlow operations for updating the adjustment values.\n",
        "  # this actually adjusts them\n",
        "  # These are basically just the reciprocal values of the\n",
        "  # loss-functions, with a small value 1e-10 added to avoid the\n",
        "  # possibility of division by zero.\n",
        "  # think of it as a biasis vector \n",
        "\n",
        "  update_adj_content = adj_content.assign(1.0 / (loss_content + 1e-10))\n",
        "  update_adj_style = adj_style.assign(1.0 / (loss_style + 1e-10))\n",
        "  update_adj_denoise = adj_denoise.assign(1.0 / (loss_denoise + 1e-10))\n",
        "\n",
        "\n",
        "  # now combine the losses \n",
        "  # This is the weighted loss-function that will be minimised\n",
        "  # below in order to generate the mixed-image.\n",
        "  # Because the loss-values is multiplied with their reciprocal\n",
        "  # adjustment values, relative weights can now be used for the\n",
        "  # loss-functions that are easier to select, as they are\n",
        "  # independent of the exact choice of style- and content-layers.\n",
        "\n",
        "  loss_combined = weight_content * adj_content * loss_content + \\\n",
        "                    weight_style * adj_style * loss_style + \\\n",
        "                    weight_denoise * adj_denoise * loss_denoise\n",
        "\n",
        "  # Now we will caalcute the gradiants using \n",
        "  # TensorFlow to get the mathematical function for the\n",
        "  # gradient is not just one, but using multiel gardainet values\n",
        "  # using the models input value and the combined loss\n",
        "  # it gets the gradient of the combined loss-function with regard to\n",
        "  # the input image. (mixed)\n",
        "  # gradient value will give us a direction , can be up or down\n",
        "  # i.e update our image up or down to minimise the loss\n",
        "\n",
        "  gradient = tf.gradients(loss_combined, model.input)\n",
        "\n",
        "  # List of tensors that we will run in each optimization iteration.\n",
        "  # this is a collection of scalar values \n",
        "  run_list = [gradient, update_adj_content, update_adj_style, \\\n",
        "                update_adj_denoise]\n",
        "  \n",
        "  # The mixed-image is initialized with random noise.\n",
        "  # It is the same size as the content-image.\n",
        "  # where we first init it\n",
        "  # np.random.rand --> initlaise an empty matrix of random values\n",
        "  mixed_image = np.random.rand(*content_image.shape) + 128\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "\n",
        "    # Create a feed-dict with the mixed-image.\n",
        "    feed_dict = model.create_feed_dict(image=mixed_image)\n",
        "\n",
        "    # Using TensorFlow --> calculate the value of the\n",
        "    # gradient, as well as updating the adjustment values.\n",
        "    # gradeint tells us how ot update out rimeg\n",
        "    # gradirent is a slope value, so given a sigmoid , the gradient is a sttrght line that either points up or down\n",
        "    # wehn gradients are taken and multipled by the image, it will update the image in someway\n",
        "    # every time the loss is minismed, the gradient is going to be \"better\"\n",
        "    # by better --> the gradient is going to minimise the difference between the output image and the style image.\n",
        "    #               everytime it will be minimum until it hits the local minimum\n",
        "    #               so the output image is goign to be the most stylised that it can be\n",
        "\n",
        "    grad, adj_content_val, adj_style_val, adj_denoise_val \\\n",
        "    = session.run(run_list, feed_dict=feed_dict)\n",
        "\n",
        "    # Reduce the dimensionality of the gradient.\n",
        "    # Remove single-dimensional entries from the shape of an array.\n",
        "    #\n",
        "    # gradient value is reduced so that it is the same size, so it can be multiped by the matrix \n",
        "    grad = np.squeeze(grad)\n",
        "\n",
        "    # Scale the step-size according to the gradient-values.\n",
        "    #\n",
        "    # same a learning rate\n",
        "    # Ratio of weights:updates\n",
        "    # akin to learning rate\n",
        "    # scaled too slowly: never going to converge\n",
        "    # sacle too fast: overfit\n",
        "    # so nee to update the gradeient and weigtj at each layer so it converages properly\n",
        "    step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
        "\n",
        "    # Update the image by following the gradient.\n",
        "    # gradient descent preformed \n",
        "    # grad--> scaler \n",
        "    # step_size_scaled--> scale vlaue\n",
        "    # gives output mixed image that will look bad at first but the more update the more styles it will be\n",
        "    mixed_image -= grad * step_size_scaled\n",
        "\n",
        "    # Ensure the image has valid pixel-values between 0 and 255.\n",
        "    # Given an interval, values outside the interval are clipped \n",
        "    # to the interval edges.\n",
        "    # clip all values that is not beteen 0- 255--> want it to be rgb\n",
        "    mixed_image = np.clip(mixed_image, 0.0, 255.0)\n",
        "\n",
        "    # Print a little progress-indicator.\n",
        "\n",
        "    print(\". \", end=\"\")\n",
        "\n",
        "    # Display status once every 10 iterations, and the last.\n",
        "    # print iterations and plot\n",
        "    if (i % 10 == 0) or (i == num_iterations - 1):\n",
        "        print()\n",
        "        print(\"Iteration:\", i)\n",
        "\n",
        "        # Print adjustment weights for loss-functions.\n",
        "        msg = \"Weight Adj. for Content: {0:.2e}, Style: {1:.2e}, Denoise: {2:.2e}\"\n",
        "        print(msg.format(adj_content_val, adj_style_val, adj_denoise_val))\n",
        "          # Plot the content-, style- and mixed-images.\n",
        "        plot_images(content_image=content_image,style_image=style_image,mixed_image=mixed_image)\n",
        "                  \n",
        "      \n",
        "   \n",
        "   print()\n",
        "   print(\"Final image\")\n",
        "   plot_image\n",
        "\n",
        "   plot_image_big(mixed_image)\n",
        "\n",
        "   # Close the TensorFlow session to release its resources.\n",
        "   session.close()\n",
        "\n",
        "   # Return the mixed-image.\n",
        "\n",
        "   return mixed_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-21c168ae188a>\"\u001b[0;36m, line \u001b[0;32m198\u001b[0m\n\u001b[0;31m    print()\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EussOlWHZ24X"
      },
      "source": [
        "# Images\n",
        "\n",
        "First load the content-image that has the overall contours that are wanted in the mixed-image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvCuYV6ngU4B"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_PcVu3gCge"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucxH5L9lTyKt"
      },
      "source": [
        "content_filename = '/content/gdrive/My Drive/StyleTrans/willy_wonka_old.jpg'\n",
        "content_image = load_image(content_filename, max_size=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpVj2LbtaKEp"
      },
      "source": [
        "Then load the stle-image: this has the colours and textures for the mixed-image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl_cMEiRaRPr"
      },
      "source": [
        "style_filename = '/content/gdrive/My Drive/StyleTrans/style7.jpg'\n",
        "style_image = load_image(style_filename, max_size=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_MJa2J5aXT2"
      },
      "source": [
        "# define list of integreed which identify the layers in the nural network that will be used for matching the conent-imgse.\n",
        "# they are indices into the layers in the neural network.\n",
        "# for the VGG16 model, the 5th layer(ndex4) seems to work well as the sole content-layer\n",
        "\n",
        "\n",
        "content_layer_ids = [4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5HGTd3a4ay"
      },
      "source": [
        "# define anotehr list of integres for the style layers\n",
        "# the VGG16-model has 13 convolutional layers.\n",
        "# this selects all those layers as the style-layers.\n",
        "# this is somewhat slow to optimise.\n",
        "\n",
        "style_layer_ids = list(range(13))\n",
        "\n",
        "# can also select a sub-set of the layers, e.g. like this:\n",
        "# style_layer_ids = [1, 2, 3, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs0PPcvke5_E"
      },
      "source": [
        "%%time\n",
        "img = style_transfer(content_image=content_image,\n",
        "                     style_image=style_image,\n",
        "                     content_layer_ids=content_layer_ids,\n",
        "                     style_layer_ids=style_layer_ids,\n",
        "                     weight_content=1.5,\n",
        "                     weight_style=10.0,\n",
        "                     weight_denoise=0.3,\n",
        "                     num_iterations=60,\n",
        "                     step_size=10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG_lXBhufAwy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx6xGWGkR3bz"
      },
      "source": [
        "To:Do learnign algothim to learn the best weights"
      ]
    }
  ]
}