{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwY1-Dv69fI8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtDgONFNdA7n"
      },
      "source": [
        "This is following:  A Neural Algorithm of Artistic Style\n",
        "\n",
        "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge\n",
        "\n",
        "--> https://arxiv.org/abs/1508.06576"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0oMWD-d9grN"
      },
      "source": [
        "Style transfer is an optimisation probelem, to  minimisation the loss function --> taking two images (style image and whatever x image ), then to minimise the difference betweeen the images. \n",
        "\n",
        "\n",
        "Zero shot or one shot learning :: machine learning with very little data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu6wl1q2_hs1"
      },
      "source": [
        "Network used is VGG16 : it is an image classification convoltionary network \n",
        "At each layer it would apply a series of operations to the input image\n",
        "Input image: matrix of values \n",
        "At each layer we have a stack of filters that are learned overtime\n",
        "\n",
        "Filters: the filters are 3D vectors. Thye are a collection of matrics that are two-d and the 3D is the rgb\n",
        "At each of these filters, the filter is multiplied by the input image. SO it is a matrox multiplication and then a summation operation.\n",
        "If it dects a fecature = very latge number\n",
        "If not = zero , therefore it is a feature identifier. \n",
        "\n",
        "It will output a feature map or an activation map ( huge matrix of values) . We want to minimise te difefre between our feature map and our content is\n",
        "\n",
        "\n",
        "Conctent image : want to modify \n",
        "Style image: we want to apply to conetcnt imag e\n",
        "Mixed Image: empty, radom nosie initilase dimage we will add to over time.\n",
        "\n",
        "There is style oss and contnet loss and combination them usign gradient value ot updat emixed image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_RSn4IY_hGb"
      },
      "source": [
        "# import our dependencies \n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt # plot out what we see\n",
        "import tensorflow as tf  # machine learnign library \n",
        "import numpy as np   # help calculate gram matrix\n",
        "import PIL.Image    # show image we want to show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1eZ5X43f-_x_",
        "outputId": "421d6bf1-abd1-442e-f0f8-0e2de7e2864e"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "ogsjQqnvCLs5",
        "outputId": "54761645-5cf2-450d-d746-4a097c9b4258"
      },
      "source": [
        "#16 layer convostuionary netwoerk. It is prerained with neccesssary filters\n",
        "# more layers means better predictions but mroe computation time \n",
        "import vgg16.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0a48e519c41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#16 layer convostuionary netwoerk. It is prerained with neccesssary filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vgg16'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4W8yh2nJ-a6",
        "outputId": "8b772eb4-d976-4862-d9fa-5f77391bea97"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16(weights='imagenet')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 8s 0us/step\n",
            "553476096/553467096 [==============================] - 8s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKFsBwg-KiR0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "vW7dd60NCiVz",
        "outputId": "d05ecf6e-b478-4ee8-a3b6-3e17832f6bf3"
      },
      "source": [
        "vgg16.maybe_download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b2c2fe046ff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'vgg16' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-MuuBxvKjNS"
      },
      "source": [
        "# define image helper fucntions \n",
        "\n",
        "def load_image(filename, max_size=None):\n",
        "    image = PIL.Image.open(filename)\n",
        "\n",
        "    if max_size is not None:\n",
        "        # Calculate the appropriate rescale-factor for\n",
        "        # ensuring a max height and width, while keeping\n",
        "        # the proportion between them.\n",
        "        factor = max_size / np.max(image.size)\n",
        "    \n",
        "        # Scale the image's height and width.\n",
        "        size = np.array(image.size) * factor\n",
        "\n",
        "        # The size is now floating-point because it was scaled.\n",
        "        # But PIL requires the size to be integers.\n",
        "        size = size.astype(int)\n",
        "\n",
        "        # Resize the image.\n",
        "        image = image.resize(size, PIL.Image.LANCZOS)\n",
        "\n",
        "    # Convert to numpy floating-point array.\n",
        "    return np.float32(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgO6WsB2KqwW"
      },
      "source": [
        "def save_image(image, filename):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "    \n",
        "    # Convert to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "    \n",
        "    # Write the image-file in jpeg-format.\n",
        "    with open(filename, 'wb') as file:\n",
        "        PIL.Image.fromarray(image).save(file, 'jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO77ylIqKw9n"
      },
      "source": [
        "def plot_image_big(image):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "\n",
        "    # Convert pixels to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Convert to a PIL-image and display it.\n",
        "    display(PIL.Image.fromarray(image))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plZ7wr9OK3_1"
      },
      "source": [
        "def plot_images(content_image, style_image, mixed_image):\n",
        "    # Create figure with sub-plots.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
        "\n",
        "    # Adjust vertical spacing.\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Use interpolation to smooth pixels?\n",
        "    smooth = True\n",
        "    \n",
        "    # Interpolation type.\n",
        "    if smooth:\n",
        "        interpolation = 'sinc'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    # Plot the content-image.\n",
        "    # Note that the pixel-values are normalized to\n",
        "    # the [0.0, 1.0] range by dividing with 255.\n",
        "    ax = axes.flat[0]\n",
        "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Content\")\n",
        "\n",
        "    # Plot the mixed-image.\n",
        "    ax = axes.flat[1]\n",
        "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Mixed\")\n",
        "\n",
        "    # Plot the style-image\n",
        "    ax = axes.flat[2]\n",
        "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Style\")\n",
        "\n",
        "    # Remove ticks from all the plots.\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHYrth0XK7tu"
      },
      "source": [
        "Loss Function \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLVZGCL3LHFT"
      },
      "source": [
        "# define mean squared error:  it is a function given to input tensors (a, b)\n",
        "# outputs a scaler --> single value\n",
        "\n",
        "# it is a hlerper fucntion inside our content loss and our style loss\n",
        "\n",
        "def mean_squared_error(a, b):\n",
        "    return tf.reduce_mean(tf.square(a - b))\n",
        "\n",
        "#it is the average of the sqaure of the difference bwteen the output feature map and our image\n",
        "# one tensor will be our content image or style image. The other will be our feature map\n",
        "                #. --> the result of the chosen layer \n",
        "# the difeeec is subtracted, the squared then we do a reduce mean\n",
        "# square makes it a positive\n",
        "\n",
        "# reduce Mean: find the average of all vlaues inside of the matrix \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJC7t0nPMb7Z"
      },
      "source": [
        "# Creates the loss-function for the content-image.\n",
        "\n",
        "'''Parameters:\n",
        "    session: An open TensorFlow session for running the model's graph.\n",
        "    model: The model, e.g. an instance of the VGG16-class.\n",
        "    content_image: Numpy float array with the content-image.\n",
        "    layer_ids: List of integer id's for the layers to use in the model. --> will be ther higher level layers\n",
        "'''\n",
        "#you should expirment what looks good for different layers\n",
        "def create_content_loss(session, model, content_image, layer_ids):\n",
        "\n",
        "  # it is a python dictionary object that is genrateed with placeholders as keys\n",
        "  # it genrates a set of place holder values \n",
        "  # key is the image, value is the content image.\n",
        "  # for very time we want to feed anything into our compeiutuon graph -->as ditionary\n",
        "   feed_dict = model.create_feed_dict(image=content_image)\n",
        "\n",
        "    # play around with differnet layers \n",
        "   layers = model.get_layer_tensors(layer_ids)\n",
        "\n",
        "  # calaute the output values given the layers --> feed into dictoanry\n",
        "  # get outptu values for thaose layers for the conent image\n",
        "   values = session.run(layers, feed_dict=feed_dict)\n",
        "\n",
        "   with model.graph.as_default():\n",
        "     # need empty as we will claaues a collection of lossses \n",
        "     layer_losses = []\n",
        "\n",
        "\n",
        "      # For each layer and its corresponding values\n",
        "     for value, layer in model(values, layers):\n",
        "\n",
        "        # tf.cocnsstnt --> cannont change\n",
        "        # this is the content image value\n",
        "        value_const = tf.constant(value)\n",
        "\n",
        "\n",
        "        #claaclte loss using mean sqaured error\n",
        "        # inputs: value at a layer, constant value\n",
        "        #\n",
        "        loss = mean_squared_error(layer, value_const)\n",
        "\n",
        "        # add to pre inititlised list \n",
        "        layer_losses.append(loss)\n",
        "\n",
        "\n",
        "     total_loss = tf.reduce_mean(layer_losses)\n",
        "\n",
        "\n",
        "   return total_loss\n",
        "\n",
        "\n",
        "# get eman sqaure eroro of feature activtions of the given layer in th model \n",
        "# between the conent image and lmixed image --.~ when minised, it will make the mixed image that much more stylised \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZY8xoAlMb-s"
      },
      "source": [
        "# for style we do a gram matrix\n",
        "# gram matrix : we are maeaurifn the correlations between out feture vectors after flattening the filter image into vectors\n",
        "# it is the matrix product bbetrwn our iniaktl matirix and then its transpoed ( filpping it by 90 degrees )\n",
        "# i.e multiplying a mtrix by its transpose \n",
        "# this is used to minimise via the mean sqaured error\n",
        "# it is 4D tensoer : collectio of pixel and rgb make it 3d -->a collection of those make it 4D\n",
        "def gram_matrix(tensor):\n",
        "\n",
        "  #4d tensor from convolutional layer\n",
        "  shape = tensor.get_shape()\n",
        "\n",
        "  # each stack in a the matrox is called a channel.\n",
        "  num_channels = int(shape[3])\n",
        "\n",
        "\n",
        "  # -1 : means whatever number makes the data fit\n",
        "  # we are reshapping th edensor soe it is a 2d matrix \n",
        "  # flatten the contents of each o the feature chnnels --> so we can multiplt it \n",
        "  # akin to normalisation \n",
        "  matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
        "\n",
        "\n",
        "    # the 2-dim matrix with itself. \n",
        "    # matrix multipled by its transpose\n",
        "  gram = tf.matmul(tf.transpose(matrix), matrix)\n",
        "\n",
        "  return gram\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLzRzkkRIiD"
      },
      "source": [
        "# style loss \n",
        "# compute given the gram matrix\n",
        "\n",
        "\n",
        "def create_style_loss(session, model, style_image, layer_ids):\n",
        "  # creates place holder value for style image\n",
        "  feed_dict = model.create_feed_dict(image=style_image)\n",
        "\n",
        "  # then get layers \n",
        "  layers = model.get_layer_tensors(layer_ids)\n",
        "\n",
        "\n",
        "  with model.graph.as_default():\n",
        "\n",
        "      # this is deffernece t the contenct loss\n",
        "      # we take the gram matrix of a layer, instead of the regular\n",
        "      # why ? ( from crator at a confernce ): it encodes 2nd order ststisics of the set of filters --> looking at it froma higehr layer\n",
        "      # gram maxtrix , tolss away anything uncewesay to focus on the syle \n",
        "      # hence it works for style and not content \n",
        "    gram_layers = [gram_matrix(layer) for layer in layers]\n",
        "\n",
        "\n",
        "    values = session.run(gram_layers, feed_dict=feed_dict)\n",
        "\n",
        "\n",
        "    # Initialize an empty list of loss-functions.\n",
        "    layer_losses = []\n",
        "\n",
        "\n",
        "    # For each Gram-matrix at each layer and its corresponding values or their loss function.\n",
        "    for value, gram_layer in model(values, gram_layers):\n",
        "\n",
        "      value_const = tf.constant(value)\n",
        "\n",
        "\n",
        "      # where calducalting the mean sqaured error of the ground layer and the value of that ground matrix layer when inputting the style image\n",
        "      loss = mean_squared_error(gram_layer, value_const)\n",
        "\n",
        "\n",
        "      # list of loss-functions --> losses at each layer\n",
        "      layer_losses.append(loss)\n",
        "    \n",
        "    # take the average of all losess in matrix --> scalar value \n",
        "    total_loss = tf.reduce_mean(layer_losses)\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AinvnPjPSvPs"
      },
      "source": [
        "# one more loss \n",
        "# the denoising loss( totoal vartioan loss) --> when paper didnt do this image was OK\n",
        "                                          # when they did it was --> result improved\n",
        "# shitfs images by one pixel on the x and y axis and calculates the diffenec btween the shifted image the original image \n",
        "# abosloute value so it is positive \n",
        "\n",
        "#--> helps supress noise in the image that was generated : its a blurriness, not that clear \n",
        "\n",
        "\n",
        "def create_denoise_loss(model):\n",
        "    loss = tf.reduce_sum(tf.abs(model.input[:,1:,:,:] - model.input[:,:-1,:,:])) + \\\n",
        "           tf.reduce_sum(tf.abs(model.input[:,:,1:,:] - model.input[:,:,:-1,:]))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B44LB9_hTmUF"
      },
      "source": [
        "# Style-transfer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bvIWxi3Tj2H"
      },
      "source": [
        "# we can modify the weigths \n",
        "# weighing the sytle more than the content\n",
        "# To:DO --> learn the best weights later with learnig algorthim \n",
        "'''\n",
        "parameters: \n",
        "    content_image: Numpy 3-dim float-array with the content-image.\n",
        "    style_image: Numpy 3-dim float-array with the style-image.\n",
        "    content_layer_ids: List of integers identifying the content-layers.\n",
        "    style_layer_ids: List of integers identifying the style-layers.\n",
        "    weight_content: Weight for the content-loss-function.\n",
        "    weight_style: Weight for the style-loss-function.\n",
        "    weight_denoise: Weight for the denoising-loss-function.\n",
        "    num_iterations: Number of optimization iterations to perform.\n",
        "    step_size: Step-size for the gradient in each iteration.\n",
        "\n",
        "'''\n",
        "def style_transfer(content_image, style_image,\n",
        "                   content_layer_ids, style_layer_ids,\n",
        "                   weight_content=1.5, weight_style=10.0,\n",
        "                   weight_denoise=0.3,\n",
        "                   num_iterations=120, step_size=10.0):\n",
        "  \n",
        "\n",
        "  #model = vgg16.VGG16()\n",
        "\n",
        "  model = VGG16()\n",
        "\n",
        "  # Create a TensorFlow-session.\n",
        "  session = tf.InteractiveSession(graph=model.graph)\n",
        "\n",
        "  # Print the names of the content-layers.\n",
        "  print(\"Content layers:\")\n",
        "  print(model.get_layer_names(content_layer_ids))\n",
        "  print()\n",
        "\n",
        "\n",
        "  # Create the loss-function for the content-layers and -image.\n",
        "  loss_content = create_content_loss(session=session,\n",
        "                                       model=model,\n",
        "                                       content_image=content_image,\n",
        "                                       layer_ids=content_layer_ids)\n",
        "  \n",
        "   # Create the loss-function for the denoising of the mixed-image.\n",
        "  loss_denoise = create_denoise_loss(model)\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucxH5L9lTyKt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}